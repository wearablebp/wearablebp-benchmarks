{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d723483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pyampd.ampd import find_peaks, find_peaks_adaptive\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e02f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data1Loader(Dataset):\n",
    "    def __init__(self, fpath, idxs, opt):\n",
    "        self.fpath = fpath\n",
    "        self.opt = opt\n",
    "        self.idxs = idxs\n",
    "        with open(fpath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.dset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # call opt = opt() beforehand\n",
    "        # returns [1, signal length, 1]\n",
    "        subj = np.fromiter(self.dset.keys(), dtype=float)[idx].astype(int).astype(str)\n",
    "        d = self.dset[subj]\n",
    "        n = np.random.randint(0, d.shape[0])\n",
    "        X1 = (d[n, 0, :] - np.median(d[n, 0, :]))/(np.percentile(d[n, 0, :], 75) - np.percentile(d[n, 0, :], 25))\n",
    "        X1 = torch.tensor(X1, dtype=torch.float32)\n",
    "        X2 = torch.tensor(d[n, 1, :], dtype=torch.float32)\n",
    "        X = torch.cat([X1.unsqueeze(0), X2.unsqueeze(0)], axis=0)\n",
    "        \n",
    "        y = d[n, 2, :]\n",
    "        try:\n",
    "            sbp_idxs = find_peaks(y)\n",
    "        except:\n",
    "            sbp_idxs = y.argmin()\n",
    "        try:\n",
    "            dbp_idxs = find_peaks(-y)\n",
    "        except:\n",
    "            dbp_idxs = y.argmin()\n",
    "        y = torch.tensor([y[sbp_idxs].mean(), y[dbp_idxs].mean()], dtype=torch.float32).to(opt.device)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d260d6b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-b568a104b592>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-b568a104b592>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    self.c =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class jeong_21(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(jeong_21, self).__init__()\n",
    "        self.c = \n",
    "        self.l1 = \n",
    "        self.l2 = \n",
    "        self.l3 = \n",
    "        self.bn = \n",
    "        self.do = \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
    "        x = x[:, 0] - x[:, 1]\n",
    "        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19e70b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 241, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 241, got 56",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-84eaa2f39584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    176\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    180\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 241, got 56"
     ]
    }
   ],
   "source": [
    "c1 = nn.Conv1d(in_channels=1, out_channels=56, kernel_size=10)\n",
    "b1 = nn.BatchNorm1d(num_features=56)\n",
    "d1 = nn.Dropout(p=0.5)\n",
    "l1 = nn.LSTM(input_size=241, hidden_size=28, num_layers=1, bidirectional=True)\n",
    "l2 = nn.LSTM(input_size=241, hidden_size=28, num_layers=1, bidirectional=True)\n",
    "\n",
    "h0 = torch.randn((2, 56, 241))\n",
    "c0 = torch.randn((2, 56, 241))\n",
    "\n",
    "a = torch.rand((4, 2, 250))\n",
    "a = (a[:, 0, :] - a[:, 1, :]).unsqueeze(1)\n",
    "a = c1(a)\n",
    "a = b1(a)\n",
    "a = d1(a).permute(0,2,1)\n",
    "print(a.shape)\n",
    "a, (hn, cn) = l1(a, (h0, c0))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3833c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(X, y, p):\n",
    "    [m_s, sd_s, m_d, sd_d] = p\n",
    "    return torch.mean(torch.abs(X[:, 0]-y[:, 0])*(1+torch.abs((y[:, 0]-m_s)/sd_s))**2) + torch.mean(torch.abs(X[:, 1]-y[:, 1])*(1+torch.abs((y[:, 1]-m_d)/sd_d))**2)\n",
    "#     return torch.mean(torch.abs(X[:, 0]-y[:, 0])/torch.tensor(norm.pdf(y[:, 0].detach().cpu(), 126, 20.5)).cuda(0)) + torch.mean(torch.abs(X[:, 1]-y[:, 1])/torch.tensor(norm.pdf(y[:, 1].detach().cpu(), 68, 12.1)).cuda(0))\n",
    "\n",
    "def loss_fn_EV(X, y):\n",
    "    return X[:, 0]-y[:, 0], X[:, 1]-y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf472cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TemporalBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-58372b9c253d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# torch.multiprocessing.set_start_method('spawn', force=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d4bf7a12e9ad>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, num_channels, kernel_size, dropout)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTCN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporalConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d4bf7a12e9ad>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_inputs, num_channels, kernel_size, dropout)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_inputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                      padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TemporalBlock' is not defined"
     ]
    }
   ],
   "source": [
    "class options:\n",
    "    def __init__(self):\n",
    "        self.numWorkers = 0\n",
    "        self.numEpoch = 50\n",
    "        self.lr = 1e-4\n",
    "        self.device = 'cuda:3'\n",
    "\n",
    "        \n",
    "        \n",
    "# dataset_names = ['data1_none', 'data1_phys',\n",
    "#                  'data1_skew_20_30', 'data1_skew_45_55', 'data1_skew_475_525', 'data1_skew_70_80', 'data1_skew_90_100',\n",
    "#                  'data1_perf_0_25', 'data1_perf_25_50', 'data1_perf_50_75', 'data1_perf_75_100',\n",
    "#                  'data1_goodness_0_25', 'data1_goodness_25_50', 'data1_goodness_50_75', 'data1_goodness_75_100']\n",
    "dataset_names = ['data1_all']\n",
    "\n",
    "start = timeit.default_timer() \n",
    "datapath = './datasets/MIMIC-II/'\n",
    "dataset_names = os.listdir(datapath)\n",
    "\n",
    "metadata = pd.DataFrame()\n",
    "for dataset_name in dataset_names:\n",
    "    \n",
    "    ### Data Loaders        \n",
    "\n",
    "    opt = options()\n",
    "    \n",
    "    fpath = datapath + dataset_name\n",
    "    with open(fpath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    idxs = np.arange(len(data.keys()))\n",
    "\n",
    "    np.random.shuffle(idxs)\n",
    "    train_idxs = idxs[np.arange(0, int(0.6*len(idxs)))]\n",
    "    val_idxs = idxs[np.arange(int(0.6*len(idxs)), int(0.8*len(idxs)))]\n",
    "    test_idxs = idxs[np.arange(int(0.8*len(idxs)), len(idxs))]\n",
    "\n",
    "    partition = {}\n",
    "    partition['train'] = train_idxs\n",
    "    partition['val'] = val_idxs\n",
    "    partition['test'] = test_idxs\n",
    "    params_train = {'batch_size': 8,\n",
    "                  'shuffle': True,\n",
    "                  'num_workers': opt.numWorkers\n",
    "                 }\n",
    "    params_val = {'batch_size': 1,\n",
    "                  'shuffle': True,\n",
    "                  'num_workers': opt.numWorkers\n",
    "                 }\n",
    "    params_test = {'batch_size': 1,\n",
    "                  'num_workers': opt.numWorkers\n",
    "                 }\n",
    "    device = torch.device(opt.device)\n",
    "    train_loader = torch.utils.data.DataLoader(Data1Loader(fpath, partition['train'], opt), **params_train)\n",
    "    val_loader = torch.utils.data.DataLoader(Data1Loader(fpath, partition['val'], opt), **params_val)\n",
    "    test_loader = torch.utils.data.DataLoader(Data1Loader(fpath, partition['test'], opt), **params_test)\n",
    "\n",
    "    ### Distribution Data\n",
    "\n",
    "    tr_sbps = np.array([])\n",
    "    tr_dbps = np.array([])\n",
    "    for _, Y_train in train_loader:\n",
    "        tr_sbps = np.append(tr_sbps, Y_train[:, 0].detach().cpu())\n",
    "        tr_dbps = np.append(tr_dbps, Y_train[:, 1].detach().cpu())\n",
    "\n",
    "    val_sbps = np.array([])\n",
    "    val_dbps = np.array([])\n",
    "    for _, Y_val in val_loader:\n",
    "        val_sbps = np.append(val_sbps, Y_val[:, 0].detach().cpu())\n",
    "        val_dbps = np.append(val_dbps, Y_val[:, 1].detach().cpu())\n",
    "\n",
    "    te_sbps = np.array([])\n",
    "    te_dbps = np.array([])\n",
    "    for _, Y_te in test_loader:\n",
    "        te_sbps = np.append(te_sbps, Y_te[:, 0].detach().cpu())\n",
    "        te_dbps = np.append(te_dbps, Y_te[:, 1].detach().cpu())\n",
    "\n",
    "    tr_sd_sbp = tr_sbps.std()\n",
    "    tr_m_sbp = tr_sbps.mean()\n",
    "    tr_sd_dbp = tr_dbps.std()\n",
    "    tr_m_dbp = tr_dbps.mean()\n",
    "\n",
    "    val_sd_sbp = val_sbps.std()\n",
    "    val_m_sbp = val_sbps.mean()\n",
    "    val_sd_dbp = val_dbps.std()\n",
    "    val_m_dbp = val_dbps.mean()\n",
    "\n",
    "    te_sd_sbp = te_sbps.std()\n",
    "    te_m_sbp = te_sbps.mean()\n",
    "    te_sd_dbp = te_dbps.std()\n",
    "    te_m_dbp = te_dbps.mean()\n",
    "\n",
    "    # fig, ax = plt.subplots(3, 2, figsize=(15,15), sharex=True)\n",
    "    # ax[0,0].hist(tr_sbps, bins=20, alpha=0.3)\n",
    "    # ax[0,0].axvline(tr_m_sbp, label='Mean SBP = ' + str(np.round(tr_m_sbp, 2)), linestyle='--', c='g')\n",
    "    # ax[0,0].axvline(tr_m_sbp+tr_sd_sbp, label='STD SBP = ±' + str(np.round(tr_sd_sbp, 2)), linestyle='--', c='r')\n",
    "    # ax[0,0].axvline(tr_m_sbp-tr_sd_sbp, linestyle='--', c='r')\n",
    "    # ax[0,0].legend()\n",
    "    # ax[0,0].set_xlabel('Training Set SBP (mmHg)')\n",
    "    # ax[0,0].set_ylabel('Count')\n",
    "    # ax[0,1].hist(tr_dbps, bins=20, alpha=0.3)\n",
    "    # ax[0,1].axvline(tr_m_dbp, label='Mean DBP = ' + str(np.round(tr_m_dbp, 2)), linestyle='--', c='g')\n",
    "    # ax[0,1].axvline(tr_m_dbp+tr_sd_dbp, label='STD DBP = ±' + str(np.round(tr_sd_dbp, 2)), linestyle='--', c='r')\n",
    "    # ax[0,1].axvline(tr_m_dbp-tr_sd_dbp, linestyle='--', c='r')\n",
    "    # ax[0,1].legend()\n",
    "    # ax[0,1].set_xlabel('Training Set DBP (mmHg)')\n",
    "    # ax[0,1].set_ylabel('Count')\n",
    "\n",
    "    # ax[1,0].hist(val_sbps, bins=20, alpha=0.3)\n",
    "    # ax[1,0].axvline(val_m_sbp, label='Mean SBP = ' + str(np.round(val_m_sbp, 2)), linestyle='--', c='g')\n",
    "    # ax[1,0].axvline(val_m_sbp+val_sd_sbp, label='STD SBP = ±' + str(np.round(val_sd_sbp, 2)), linestyle='--', c='r')\n",
    "    # ax[1,0].axvline(val_m_sbp-val_sd_sbp, linestyle='--', c='r')\n",
    "    # ax[1,0].legend()\n",
    "    # ax[1,0].set_xlabel('Validation Set SBP (mmHg)')\n",
    "    # ax[1,0].set_ylabel('Count')\n",
    "    # ax[1,1].hist(val_dbps, bins=20, alpha=0.3)\n",
    "    # ax[1,1].axvline(val_m_dbp, label='Mean DBP = ' + str(np.round(val_m_dbp, 2)), linestyle='--', c='g')\n",
    "    # ax[1,1].axvline(val_m_dbp+val_sd_dbp, label='STD DBP = ±' + str(np.round(val_sd_dbp, 2)), linestyle='--', c='r')\n",
    "    # ax[1,1].axvline(val_m_dbp-val_sd_dbp, linestyle='--', c='r')\n",
    "    # ax[1,1].legend()\n",
    "    # ax[1,1].set_xlabel('Validation Set DBP (mmHg)')\n",
    "    # ax[1,1].set_ylabel('Count')\n",
    "\n",
    "    # ax[2,0].hist(te_sbps, bins=20, alpha=0.3)\n",
    "    # ax[2,0].axvline(te_m_sbp, label='Mean SBP = ' + str(np.round(te_m_sbp, 2)), linestyle='--', c='g')\n",
    "    # ax[2,0].axvline(te_m_sbp+te_sd_sbp, label='STD SBP = ±' + str(np.round(te_sd_sbp, 2)), linestyle='--', c='r')\n",
    "    # ax[2,0].axvline(te_m_sbp-te_sd_sbp, linestyle='--', c='r')\n",
    "    # ax[2,0].legend()\n",
    "    # ax[2,0].set_xlabel('Testing Set SBP (mmHg)')\n",
    "    # ax[2,0].set_ylabel('Count')\n",
    "    # ax[2,1].hist(te_dbps, bins=20, alpha=0.3)\n",
    "    # ax[2,1].axvline(te_m_dbp, label='Mean DBP = ' + str(np.round(te_m_dbp, 2)), linestyle='--', c='g')\n",
    "    # ax[2,1].axvline(te_m_dbp+te_sd_dbp, label='STD DBP = ±' + str(np.round(te_sd_dbp, 2)), linestyle='--', c='r')\n",
    "    # ax[2,1].axvline(te_m_dbp-te_sd_dbp, linestyle='--', c='r')\n",
    "    # ax[2,1].legend()\n",
    "    # ax[2,1].set_xlabel('Testing Set DBP (mmHg)')\n",
    "    # ax[2,1].set_ylabel('Count')\n",
    "\n",
    "    # Req BP Distribution SBP STD:  21.121632046210227\n",
    "    # Req BP Distribution DBP STD:  12.310057157454578\n",
    "\n",
    "\n",
    "\n",
    "    ### Training\n",
    "\n",
    "    # torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    model = TCN(2, 2, [25]*8, 7, 0.0).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_dir = './models'\n",
    "    model_name = 'best_model_' + 'tcn1_' + dataset_name\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    try:\n",
    "        EPOCHS = 10\n",
    "        best_vloss = 1e10\n",
    "        for epoch in tqdm(range(EPOCHS)):\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            last_loss = 0.0\n",
    "\n",
    "            i = 0\n",
    "            for X_train, y_train in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_train.to(device))\n",
    "                loss = loss_fn(outputs, y_train.cuda(0), [tr_m_sbp, tr_sd_sbp, tr_m_dbp, tr_sd_dbp])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                i+=1\n",
    "                running_loss += loss.item()\n",
    "                if i % 10 == 9:\n",
    "                    last_loss = running_loss / 10 # loss per batch\n",
    "                    print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                    tb_x = epoch_number * len(train_loader) + i + 1\n",
    "                    writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                    running_loss = 0.0\n",
    "            avg_loss = last_loss\n",
    "\n",
    "            i=0\n",
    "            model.eval()    \n",
    "            running_vloss = 0.0\n",
    "            vlosses_s = np.array([])\n",
    "            vlosses_d = np.array([])\n",
    "            for X_val, y_val in val_loader:\n",
    "                voutputs = model(X_val.to(device))\n",
    "                vloss = loss_fn(voutputs, y_val, [val_m_sbp, val_sd_sbp, val_m_dbp, val_sd_dbp])\n",
    "                vloss_s, vloss_d = loss_fn_EV(voutputs, y_val)\n",
    "                vlosses_s = np.append(vlosses_s, vloss_s.detach().cpu())\n",
    "                vlosses_d = np.append(vlosses_d, vloss_d.detach().cpu())\n",
    "                running_vloss += vloss\n",
    "                i+=1\n",
    "            EVd = (val_sd_dbp/vlosses_d.std())**2\n",
    "            EVs = (val_sd_sbp/vlosses_s.std())**2\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            print('train_loss = {} \\n valid_loss = {} \\n EVs = {} \\n EVd = {}'.format(avg_loss, avg_vloss, EVs, EVd))\n",
    "\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch_number + 1)\n",
    "            writer.flush()\n",
    "\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = model_dir + os.sep + model_name\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            epoch_number += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### Testing\n",
    "\n",
    "        saved_model = TCN(2, 2, [25]*8, 7, 0.05).to(device)\n",
    "        saved_model.load_state_dict(torch.load(model_path))\n",
    "        saved_model.eval()\n",
    "\n",
    "        i=0\n",
    "        running_te_loss = 0.0\n",
    "        running_s_loss = 0.0\n",
    "        running_d_loss = 0.0\n",
    "        te_losses_s = np.array([])\n",
    "        te_losses_d = np.array([])\n",
    "        est_s = np.array([])\n",
    "        est_d = np.array([])\n",
    "        gt_s = np.array([])\n",
    "        gt_d = np.array([])\n",
    "        for X_te, y_te in test_loader:\n",
    "            te_outputs = model(X_te.to(device))\n",
    "            te_loss = loss_fn(te_outputs, y_te, [te_m_sbp, te_sd_sbp, te_m_dbp, te_sd_dbp])\n",
    "            te_loss_s, te_loss_d = loss_fn_EV(te_outputs, y_te)\n",
    "            te_losses_s = np.append(te_losses_s, te_loss_s.detach().cpu())\n",
    "            te_losses_d = np.append(te_losses_d, te_loss_d.detach().cpu())\n",
    "            est_s = np.append(est_s, te_outputs[0, 0].detach().cpu())\n",
    "            est_d = np.append(est_d, te_outputs[0, 1].detach().cpu())\n",
    "            gt_s = np.append(gt_s, y_te[0, 0].detach().cpu())\n",
    "            gt_d = np.append(gt_d, y_te[0, 1].detach().cpu())\n",
    "            running_te_loss += te_loss\n",
    "            i+=1\n",
    "        te_EVd = (te_sd_dbp/te_losses_d.std())**2\n",
    "        te_EVs = (te_sd_sbp/te_losses_s.std())**2\n",
    "        avg_te_loss = running_te_loss.detach().cpu().item() / (i + 1)\n",
    "        avg_s_loss = abs(te_losses_s).mean()\n",
    "        avg_d_loss = abs(te_losses_d).mean()\n",
    "        print('test_loss = {} \\n sbp_loss = {} \\n dbp_loss = {} \\n EVs = {} \\n EVd = {}'.format(avg_te_loss, avg_s_loss, avg_d_loss, te_EVs, te_EVd))\n",
    "\n",
    "        p = np.polyfit(gt_s, est_s-gt_s, deg=1)\n",
    "        # fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        # ax[0].scatter(gt_s, est_s-gt_s)\n",
    "        # ax[0].set_xlabel('Reference SBP (mmHg)')\n",
    "        # ax[0].set_ylabel('SBP Error (mmHg)')\n",
    "        # ax[1].scatter(gt_d, est_d-gt_d)\n",
    "        # ax[1].set_xlabel('Reference DBP (mmHg)')\n",
    "        # ax[1].set_ylabel('DBP Error (mmHg)')\n",
    "\n",
    "        # plt.scatter(est_s, gt_s)\n",
    "        # plt.xlim([gt_s.min(), gt_s.max()])\n",
    "        # plt.ylim([gt_s.min(), gt_s.max()])       \n",
    "\n",
    "        metadata = pd.concat([metadata, pd.DataFrame(data = [[dataset_name, len(test_loader), te_m_sbp, te_sd_sbp, te_m_dbp, te_sd_dbp, p[0], p[1], avg_te_loss, avg_s_loss, avg_d_loss, te_EVs, te_EVd]], columns=['dataset', 'num_samples', 'm_sbp', 'sd_sbp', 'm_dbp', 'sd_dbp', 'p0', 'p1', 'test loss', 'SBP MAE', 'DBP MAE', 'EV SBP', 'EV DBP'])])\n",
    "        print(metadata)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d541003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TCN'\n",
    "metadata.to_csv('./results/' + model_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bcf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
