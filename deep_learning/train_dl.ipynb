{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15a11c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install PyWavelets\n",
    "!pip install dill\n",
    "!pip install einops\n",
    "!pip install neurokit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d723483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "from torch.nn.utils import weight_norm\n",
    "import h5py\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torch import einsum\n",
    "\n",
    "from dl_dataloaders import *\n",
    "from dl_models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class options:\n",
    "    def __init__(self):\n",
    "        self.device ='cuda:3'\n",
    "        \n",
    "        self.data_dirpath = '../../datasets/vitaldb/'\n",
    "        self.dataset_filename = 'vitaldb'    \n",
    "        self.dataset_name = 'vitaldb'\n",
    "        self.filter_name = 'zhang21'\n",
    "        self.alg_name = 'jeong21'\n",
    "        \n",
    "        self.numWorkers = 0\n",
    "        self.numEpoch = 20\n",
    "        self.bs = 28\n",
    "        \n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = torch.optim.Adam\n",
    "        self.lr = 1e-3\n",
    "        self.opt_wdecay = 2e-5\n",
    "        \n",
    "        self.train_pct = 0.6\n",
    "        self.val_pct = 0.2\n",
    "        self.test_pct = 0.2\n",
    "        \n",
    "        self.dataloader = jeong21_dataloader\n",
    "        self.model = jeong21\n",
    "        if self.model == jeong21:\n",
    "            self.model_params = {}\n",
    "            self.optimizer_params = {'lr':self.lr, 'weight_decay':self.opt_wdecay}\n",
    "            self.schedule = identity\n",
    "            self.schedule_params = {}\n",
    "        elif self.model == huang22:\n",
    "            self.model_params = {'in_channel':12, 'num_patches':128, 'num_classes':2,\n",
    "                                   'dim':256, 'depth':6, 'attn_dim':64}\n",
    "            self.optimizer_params = {'betas':(0.9, 0.98), 'eps':1e-09}\n",
    "            self.schedule = identity\n",
    "            self.schedule_params = {}\n",
    "#             self.schedule = ScheduledOptim\n",
    "#             self.schedule_params = {'d_model':1000, 'n_warmup_steps':800}\n",
    "        \n",
    "        self.sbp_model_names = [self.alg_name]\n",
    "        self.dbp_model_names = [self.alg_name]\n",
    "        \n",
    "def identity(x):\n",
    "    return x\n",
    "        \n",
    "opt = options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf472cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "metadata = pd.DataFrame()  \n",
    "    \n",
    "fpath = opt.data_dirpath + opt.dataset_filename + '.h5'\n",
    "with h5py.File(fpath, 'r') as f:\n",
    "    data = f\n",
    "    idxs = np.arange(len(data.keys()))\n",
    "    np.random.shuffle(idxs)\n",
    "    ks = np.array(list(data.keys()))\n",
    "    train_idxs = ks[idxs[np.arange(0, int(opt.train_pct*len(idxs)))]]\n",
    "    val_idxs = ks[idxs[np.arange(int(opt.train_pct*len(idxs)), int((opt.train_pct + opt.val_pct)*len(idxs)))]]\n",
    "    test_idxs = ks[idxs[np.arange(int((opt.train_pct + opt.val_pct)*len(idxs)), len(idxs))]]\n",
    "\n",
    "partition = {}\n",
    "partition['train'] = train_idxs\n",
    "partition['val'] = val_idxs\n",
    "partition['test'] = test_idxs\n",
    "params_train = {'batch_size': opt.bs,\n",
    "              'shuffle': True,\n",
    "              'num_workers': opt.numWorkers\n",
    "             }\n",
    "params_val = {'batch_size': 1,\n",
    "              'shuffle': True,\n",
    "              'num_workers': opt.numWorkers\n",
    "             }\n",
    "params_test = {'batch_size': 1,\n",
    "              'num_workers': opt.numWorkers\n",
    "             }\n",
    "train_loader = torch.utils.data.DataLoader(opt.dataloader(fpath, partition['train'], opt), **params_train)\n",
    "val_loader = torch.utils.data.DataLoader(opt.dataloader(fpath, partition['val'], opt), **params_val)\n",
    "test_loader = torch.utils.data.DataLoader(opt.dataloader(fpath, partition['test'], opt), **params_test)\n",
    "\n",
    "### Compute Data Distribution\n",
    "# tr_sbps = np.array([])\n",
    "# tr_dbps = np.array([])\n",
    "val_sbps = np.array([])\n",
    "val_dbps = np.array([])\n",
    "te_sbps = np.array([])\n",
    "te_dbps = np.array([])\n",
    "# for _, y_train in train_loader:\n",
    "#     tr_sbps = np.append(tr_sbps, y_train[:, 0].detach().cpu())\n",
    "#     tr_dbps = np.append(tr_dbps, y_train[:, 1].detach().cpu())\n",
    "for _, y_val in val_loader:\n",
    "    val_sbps = np.append(val_sbps, y_val[:, 0].detach().cpu())\n",
    "    val_dbps = np.append(val_dbps, y_val[:, 1].detach().cpu())\n",
    "for _, y_test in test_loader:\n",
    "    te_sbps = np.append(te_sbps, y_test[:, 0].detach().cpu())\n",
    "    te_dbps = np.append(te_dbps, y_test[:, 1].detach().cpu())\n",
    "# tr_sd_sbp = tr_sbps.std()\n",
    "# tr_m_sbp = tr_sbps.mean()\n",
    "# tr_sd_dbp = tr_dbps.std()\n",
    "# tr_m_dbp = tr_dbps.mean()\n",
    "val_sd_sbp = val_sbps.std()\n",
    "val_m_sbp = val_sbps.mean()\n",
    "val_sd_dbp = val_dbps.std()\n",
    "val_m_dbp = val_dbps.mean()\n",
    "te_sd_sbp = te_sbps.std()\n",
    "te_m_sbp = te_sbps.mean()\n",
    "te_sd_dbp = te_dbps.std()\n",
    "te_m_dbp = te_dbps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c9508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Training\n",
    "# torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "model = opt.model(**opt.model_params).to(opt.device)\n",
    "optimizer = opt.schedule(opt.optimizer(model.parameters(), **opt.optimizer_params), **opt.schedule_params)\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_dir = './models'\n",
    "save_model_name = opt.dataset_name + '_' + opt.filter_name + '_' + opt.alg_name\n",
    "writer = SummaryWriter('./runs/' + save_model_name + '_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = opt.numEpoch\n",
    "best_vloss = 1e10\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    i = 0\n",
    "    for X_train, y_train in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.float().to(opt.device))\n",
    "        loss = opt.loss_fn(outputs, y_train.to(opt.device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_number * len(train_loader) + i + 1\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "    avg_loss = last_loss\n",
    "\n",
    "    model.eval()    \n",
    "    vlosses_s = np.array([])\n",
    "    vlosses_d = np.array([])\n",
    "    running_vloss = 0.0\n",
    "    for X_val, y_val in val_loader:\n",
    "        voutputs = model(X_val.float().to(opt.device))\n",
    "        vloss = opt.loss_fn(voutputs, y_val)\n",
    "        vloss_s, vloss_d = me_loss(voutputs, y_val)\n",
    "        vlosses_s = np.append(vlosses_s, vloss_s.detach().cpu().numpy())\n",
    "        vlosses_d = np.append(vlosses_d, vloss_d.detach().cpu().numpy())\n",
    "        running_vloss += vloss.item()\n",
    "    EDd = val_sd_dbp/vlosses_d.std()\n",
    "    EDs = val_sd_sbp/vlosses_s.std()\n",
    "    avg_vloss = running_vloss/len(val_loader)/params_val['batch_size']\n",
    "    print('train_loss = {} \\n valid_loss = {} \\n EDs = {} \\n EDd = {}'.format(avg_loss, avg_vloss, EDs, EDd))\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = model_dir + os.sep + opt.dataset_name + '_' + opt.filter_name + '_' + opt.alg_name\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c8ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Testing\n",
    "\n",
    "opt.device= 'cpu'\n",
    "# model = opt.model(**opt.model_params).to(opt.device)\n",
    "model_dir = './models'\n",
    "model_path = model_dir + os.sep + opt.dataset_name + '_' + opt.filter_name + '_' + opt.alg_name\n",
    "saved_model = opt.model(**opt.model_params).to(opt.device)\n",
    "saved_model.load_state_dict(torch.load(model_path))\n",
    "saved_model.eval() \n",
    "\n",
    "running_te_loss = 0.0\n",
    "running_s_loss = 0.0\n",
    "running_d_loss = 0.0\n",
    "te_losses_s = np.array([])\n",
    "te_losses_d = np.array([])\n",
    "sbp_ests = np.array([])\n",
    "dbp_ests = np.array([])\n",
    "sbp_gts = np.array([])\n",
    "dbp_gts = np.array([])\n",
    "for i in range(0, 10):\n",
    "    i=0\n",
    "    for X_test, y_test in test_loader:\n",
    "        te_outputs = saved_model(X_test.float().to(opt.device))\n",
    "        te_loss = opt.loss_fn(te_outputs, y_test)\n",
    "        te_loss_s, te_loss_d = me_loss(te_outputs, y_test)\n",
    "        te_losses_s = np.append(te_losses_s, te_loss_s.detach().cpu().numpy())\n",
    "        te_losses_d = np.append(te_losses_d, te_loss_d.detach().cpu().numpy())\n",
    "        sbp_ests = np.append(sbp_ests, te_outputs[0, 0].detach().cpu().numpy())\n",
    "        dbp_ests = np.append(dbp_ests, te_outputs[0, 1].detach().cpu().numpy())\n",
    "        sbp_gts = np.append(sbp_gts, y_test[0, 0].detach().cpu().numpy())\n",
    "        dbp_gts = np.append(dbp_gts, y_test[0, 1].detach().cpu().numpy())\n",
    "        running_te_loss += te_loss\n",
    "        i+=1\n",
    "te_EDd = te_sd_dbp/te_losses_d.std()\n",
    "te_EDs = te_sd_sbp/te_losses_s.std()\n",
    "avg_te_loss = running_te_loss.detach().cpu().item() / (i + 1)\n",
    "avg_s_loss = abs(te_losses_s).mean()\n",
    "avg_d_loss = abs(te_losses_d).mean()\n",
    "print('test_loss = {} \\n sbp_loss = {} \\n dbp_loss = {} \\n EDs = {} \\n EDd = {}'.format(avg_te_loss, avg_s_loss, avg_d_loss, te_EDs, te_EDd))     \n",
    "\n",
    "sbp_model_results = {}\n",
    "dbp_model_results = {}\n",
    "sbp_std = te_sd_sbp\n",
    "dbp_std = te_sd_dbp\n",
    "sbp_model_result = {}\n",
    "sbp_model_result['raw ests'] = sbp_ests\n",
    "sbp_model_result['raw gts'] = sbp_gts\n",
    "sbp_model_result['bias'] = (sbp_ests - sbp_gts).mean()\n",
    "sbp_model_result['err std'] = (sbp_ests - sbp_gts).std()\n",
    "sbp_model_result['ED'] = sbp_std/(sbp_ests - sbp_gts).std()\n",
    "sbp_model_result['dist std'] = sbp_std\n",
    "sbp_model_results[opt.alg_name] = sbp_model_result\n",
    "dbp_model_result = {}\n",
    "dbp_model_result['raw ests'] = dbp_ests\n",
    "dbp_model_result['raw gts'] = dbp_gts\n",
    "dbp_model_result['bias'] = (dbp_ests - dbp_gts).mean()\n",
    "dbp_model_result['err std'] = (dbp_ests - dbp_gts).std()\n",
    "dbp_model_result['ED'] = dbp_std/(dbp_ests - dbp_gts).std()\n",
    "sbp_model_result['dist std'] = sbp_std\n",
    "dbp_model_results[opt.alg_name] = dbp_model_result\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dict = {'sbp': sbp_model_results, 'dbp': dbp_model_results, 'opt': opt}\n",
    "with open('../results/training/' + opt.dataset_name + '_' + opt.filter_name + '_' + opt.alg_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump(pickle_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c738cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
