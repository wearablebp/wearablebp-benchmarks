{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8d6dc4",
   "metadata": {},
   "source": [
    "This notebook contains examples of how to convert different datasets into .h5 format. We choose to use the .h5 format because data can be accessed far quicker than other formats that require loading the whole dataset into memory. For more information about available datasets, see our [website page](https://wearablebp.github.io/datasets) on publicly available datasets.\n",
    "\n",
    "The examples below creates segments of arbitrary size. Each reshaped record is stacked together in the form [segment number, sensor data number, segment length]. For example, in the MIMIC-II dataset from UCI Repository, record 0 will be saved in the same [21, 3, 128] where 21 is the number of segments, 3 is the number of signals (ECG, PPG, ABP), and 128 is the segment length. Data can be retrieved by specifying '<record number>' and indexing (example: `hf['0'][0, :, :]`). The record numbers can be viewed using the command `hf.keys()`.   \n",
    "   \n",
    "1. [MIMIC-II from UCI Repository](#mimic)\n",
    "2. [PPG-BP](#ppgbp)\n",
    "3. [University of Queensland Vital Signs Database](#uoq)\n",
    "4. [PTT-PPG](#pttpgg)\n",
    "5. [CHARIS](#charis)\n",
    "6. [HYPE](#hype)\n",
    "7. [Non-Invasive Blood Pressure Estimation](#nibp)\n",
    "8. [VitalDB](#vitaldb)\n",
    "9. [Aurora-BP](#aurora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special packages used for particular datasets\n",
    "!pip install openpyxl\n",
    "!pip install devicely\n",
    "!pip install glob2\n",
    "!pip install wfdb\n",
    "!pip install vitaldb\n",
    "\n",
    "import glob\n",
    "import devicely\n",
    "import vitaldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9de061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_dset(datapath):\n",
    "    dset = h5py.File(datapath, 'r')\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93aeac",
   "metadata": {},
   "source": [
    "# MIMIC-II from UCI Repository ([dataset link](https://archive.ics.uci.edu/ml/datasets/Cuff-Less+Blood+Pressure+Estimation), [paper link](https://ieeexplore.ieee.org/document/7491263))\n",
    "\n",
    "\n",
    "\n",
    "Contains raw ECG, PPG, and ABP signals from the physionet [MIMIC-II Waveform Database](https://archive.physionet.org/physiobank/database/mimic2wdb/) processed by [Kachuee et al., (2015)](https://ieeexplore.ieee.org/document/7491263). Which signals contain signals from the same patients is unclear. In the paper, approximately 1 in 12 signals come from the same person. A [follow up work](https://ieeexplore.ieee.org/document/8938751) with a co-author also used this dataset and reduces the data leakage by shuffling and random sampling.\n",
    "\n",
    "<a id='mimic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../datasets/MIMIC-II/Part1234.mat'\n",
    "dset = load_h5_dset(datapath)\n",
    "segment_length = 128\n",
    "\n",
    "with h5py.File('../../datasets/MIMIC-II/kachuee17_' + str(int(segment_length)) + '.h5', 'w') as hf:\n",
    "    keys = list(dset.keys())\n",
    "    for k in range(len(keys)):\n",
    "        subj_data = []\n",
    "        if dset[keys[k]][:].shape[1] >= segment_length:\n",
    "            for i in range(0, segment_length*(dset[keys[k]][:].shape[1]//segment_length), segment_length):\n",
    "                subj_data.append(dset[keys[k]][:, i:i+segment_length])\n",
    "        if len(subj_data) > 0:\n",
    "            hf.create_dataset(keys[k], data=np.array(subj_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../../datasets/MIMIC-II/kachuee17_1000.h5', 'r') as hf:\n",
    "    print(hf['0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5837c",
   "metadata": {},
   "source": [
    "# PPG-BP Dataset ([dataset link](https://figshare.com/articles/dataset/PPG-BP_Database_zip/5459299), [paper link](https://www.nature.com/articles/sdata201820))\n",
    "\n",
    "Contains ~2.1s PPG data and cuff BP measurements and demographic information. Cuff BP measurements are contained in 'Table 1.xlsx'. The PPG data is filtered using SQI metrics from [Liang et al., (2018)](https://www.nature.com/articles/sdata201876).\n",
    "\n",
    "Because .h5 files store data in arrays, the data need to be matched in all dimensions. Since cuff BP measurements are only reported once for each subject, an array of size <segment length> is created with the first half of values as SBP and second half of values as DBP.\n",
    "\n",
    "<a id='ppgbp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppgbp_to_h5(dirpath):\n",
    "\n",
    "    fnames = os.listdir(dirpath)\n",
    "    subjs = []\n",
    "    for fname in fnames:\n",
    "        subjs.append(fname.split('_')[0])\n",
    "    unique_subjs = np.unique(subjs)\n",
    "\n",
    "    gt = pd.read_excel(dirpath + '../' + 'PPG-BP dataset.xlsx', header=1)\n",
    "    subj_ids = gt['subject_ID'].astype(str).values\n",
    "    sbp_gt = gt['Systolic Blood Pressure(mmHg)'].values\n",
    "    dbp_gt = gt['Diastolic Blood Pressure(mmHg)'].values\n",
    "\n",
    "    with h5py.File('../../datasets/PPG-BP/liang18.h5', 'w') as hf:\n",
    "        for subj in unique_subjs:\n",
    "            subj_data = np.array([])\n",
    "            for filename in os.listdir(dirpath):\n",
    "                s = filename.split('_')\n",
    "                if s[0] == subj:\n",
    "                    record = s[1].split('.')[0]\n",
    "                    d = np.loadtxt(dirpath + filename)\n",
    "                    d = np.expand_dims(d, axis=0)\n",
    "                    s = d.shape\n",
    "                    d = np.stack((d, np.hstack((np.ones((1, s[1]//2))*sbp_gt[np.where(subj_ids == subj)], np.ones((1, s[1]//2))*dbp_gt[np.where(subj_ids == subj)]))), axis=1)\n",
    "                    if ~subj_data.any():\n",
    "                        subj_data = d\n",
    "                    else:\n",
    "                        if subj_data.shape[1] == len(d):\n",
    "                            subj_data = np.vstack((subj_data, d))\n",
    "                        else:\n",
    "                            for i in range(0, len(d), 2100):\n",
    "                                subj_data = np.vstack((subj_data, d[:, :, i:i+2100]))\n",
    "            hf.create_dataset(subj, data=subj_data)\n",
    "    \n",
    "dirpath = '../../datasets/PPG-BP/0_subject/'\n",
    "ppgbp_to_h5(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../datasets/PPG-BP/liang18.h5'\n",
    "dset = load_h5_dset(datapath)\n",
    "dset['10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252de32c",
   "metadata": {},
   "source": [
    "# University of Queensland Vital Signs Database ([dataset link](https://outbox.eait.uq.edu.au/uqdliu3/uqvitalsignsdataset/index.html), [paper link](https://journals.lww.com/anesthesia-analgesia/Fulltext/2012/03000/University_of_Queensland_Vital_Signs_Dataset_.15.aspx))\n",
    "\n",
    "Contains vital signs data of 32 surgical cases where patients underwent at the Royal Adelaide Hospital. Compared to MIMIC, UoQ provides more complete dataset with simultaneous and synchronized recording of multiple vital sign parameters. Contains ECG, PPG, ABP signals and more.\n",
    "\n",
    "<a id='uoq'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce257519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uoq_to_h5(dirpath):\n",
    "    \n",
    "    numeric_cols = np.array([])\n",
    "    for case in os.listdir(dirpath):\n",
    "        if 'case' in case:\n",
    "            fulldata_dir = dirpath + case + '/fulldata/'\n",
    "            subj_data = np.array([])\n",
    "            for f in os.listdir(fulldata_dir):\n",
    "                d = pd.read_csv(fulldata_dir + f, error_bad_lines=False)\n",
    "                columns = d.columns\n",
    "                d = d._get_numeric_data()\n",
    "                numeric_cols = np.append(numeric_cols, d.columns)\n",
    "    numeric_cols = np.unique(numeric_cols)\n",
    "    \n",
    "    hf = h5py.File(dirpath + 'uoq_dset.h5', 'w')\n",
    "    for case in os.listdir(dirpath):\n",
    "        if 'case' in case:\n",
    "            fulldata_dir = dirpath + case + '/fulldata/'\n",
    "            for f in os.listdir(fulldata_dir):\n",
    "                d = pd.read_csv(fulldata_dir + f, error_bad_lines=False)\n",
    "                columns = d.columns\n",
    "                d = d[numeric_cols]\n",
    "                for col in d.columns:\n",
    "                    d[col] = pd.to_numeric(d[col], errors='coerce')\n",
    "            hf.create_dataset(case + '_' + f.split('_')[-1].split('.')[0], data=d.T)\n",
    "    hf.close()\n",
    "\n",
    "dirpath = '../../datasets/uqvitalsignsdata/'\n",
    "uoq_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/uqvitalsignsdata/uoq_dset.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34154635",
   "metadata": {},
   "source": [
    "# PTT-PPG ([dataset link](https://physionet.org/content/pulse-transit-time-ppg/1.0.0/))\n",
    "\n",
    "Contains time synchronised (and some multi-site) signals worn at different body locations including PPG, gyroscope, cuff BP, and ECG from 22 healthy subjects performing 3 physical activities. ECG data is also annotated. Also includes SpO2 from PPG.\n",
    "\n",
    "<a id='pttppg'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pttppg_to_h5(dirpath)\n",
    "    numeric_cols = np.array([])\n",
    "    for s in unique_subjs:\n",
    "        for f in unique_fnames:\n",
    "            if 's' + s + '_' in f:\n",
    "                d = pd.read_csv(dirpath + f)\n",
    "                d = d._get_numeric_data()\n",
    "                columns = d.columns\n",
    "            numeric_cols = np.append(numeric_cols, d.columns)\n",
    "    numeric_cols = np.unique(numeric_cols)\n",
    "\n",
    "    hf = h5py.File(dirpath + 'pttppg.h5', 'w')\n",
    "    for s in unique_subjs:\n",
    "        for f in unique_fnames:\n",
    "            if 's' + s + '_' in f:\n",
    "                d = pd.read_csv(dirpath + f)\n",
    "                d = d[numeric_cols]\n",
    "                for col in d.columns:\n",
    "                    d[col] = pd.to_numeric(d[col], errors='coerce')\n",
    "                hf.create_dataset(f.split('.')[0], data=d.T)\n",
    "    hf.close()  \n",
    "    return numeric_cols\n",
    "\n",
    "dirpath = '../../datasets/pulse-transit-time-ppg/1.1.0/csv/'\n",
    "cols = pttppg_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/pulse-transit-time-ppg/1.1.0/csv/pttppg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a75b9",
   "metadata": {},
   "source": [
    "# CHARIS ([dataset link](https://physionet.org/content/charisdb/1.0.0/), [paper link](https://link.springer.com/article/10.1007/s10877-015-9779-3))\n",
    "\n",
    "Contains multi-channel recordings of ECG, arterial blood pressure (ABP), and intracranial pressure (ICP) of 29 patients diagnosed with traumatic brain injury (TBI) over a 18-month period.\n",
    "\n",
    "<a id=\"charis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd95b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charis_to_h5(dirpath):\n",
    "    fnames = np.array([])\n",
    "    for f in os.listdir(dirpath):\n",
    "        if ('charis' in f) & ('h5' not in f):\n",
    "            fnames = np.append(fnames, f.split('.')[0])\n",
    "    unique_fnames = np.unique(fnames)\n",
    "\n",
    "    hf = h5py.File(dirpath + 'charis.h5', 'w')\n",
    "    for f in unique_fnames:\n",
    "        if ('charis' in f) & ('h5' not in f):\n",
    "            signals, fields = wfdb.rdsamp(dirpath + f.split('.')[0])\n",
    "            hf.create_dataset(f.split('.')[0].split('charis')[1], data=signals.T)\n",
    "    hf.close()\n",
    "    \n",
    "dirpath = '../../datasets/charisdb/1.0.0/'\n",
    "charis_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/charisdb/1.0.0/charis.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16027905",
   "metadata": {},
   "source": [
    "# HYPE ([dataset request form](https://docs.google.com/forms/d/e/1FAIpQLSe9ak7gqdGeRhhvG5Z3DqsyLxvfUcQ3Ktbs7wFfay7VxmU9ag/viewform), [paper link](https://link.springer.com/chapter/10.1007/978-3-030-59137-3_29), [GitHub repo](https://github.com/arianesasso/aime-2020))\n",
    "\n",
    "Contains PPG signals from Empatica E4 Watch for subjects performing stress tests (N=8) and over 24 hours (N=9) with cuff BP reference. Also included Age, Gender, and BMI information.\n",
    "\n",
    "<a id=\"hype\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process stress test data\n",
    "def hype_to_h5(dirpath):\n",
    "    hf = h5py.File(dirpath + 'hype.h5', 'w')\n",
    "    for s in os.listdir(dirpath):\n",
    "        if ('h5' not in s) & ('DS' not in s):\n",
    "            patient_base_path = dirpath + s\n",
    "            sources = {}\n",
    "            if os.path.exists(patient_base_path+'/Tag'):\n",
    "                sources['tag'] = glob.glob(patient_base_path+r'/Tag*').pop()\n",
    "            if os.path.exists(patient_base_path+'/Empatica'):\n",
    "                sources['empatica'] = glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "            if os.path.exists(patient_base_path+'/SpaceLabs'):\n",
    "                sources['spacelabs'] = glob.glob(patient_base_path+r'/SpaceLabs*').pop() \n",
    "            \n",
    "            if len(sources) > 0:\n",
    "                empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "                if os.path.exists(sources['spacelabs']):\n",
    "                    for file in os.listdir(sources['spacelabs']):\n",
    "                        if file.endswith(\".abp\"):\n",
    "                            spacelabsfile = os.path.join(sources['spacelabs'], file)\n",
    "                            break\n",
    "        #         print(spacelabsfile)\n",
    "                bp = devicely.SpacelabsReader(spacelabsfile)\n",
    "            #     bp.drop_EB()\n",
    "                bp.timeshift(pd.Timedelta(-2, unit='H'))\n",
    "\n",
    "                edata = empatica.data._get_numeric_data()\n",
    "                bpdata = bp.data._get_numeric_data()\n",
    "                subj_data = pd.DataFrame()\n",
    "                for i in range(len(bpdata.index)-1):\n",
    "                    e = edata[(edata.index >= bpdata.index[i]) & (edata.index < bpdata.index[i+1])]\n",
    "                    for col in bpdata.iloc[i].index:\n",
    "                        e[col] = bpdata.iloc[i][col]\n",
    "                        columns = e.columns\n",
    "                    subj_data = subj_data.append(e)\n",
    "                hf.create_dataset(s, data=subj_data.T)\n",
    "    return columns\n",
    "\n",
    "dirpath = '../../datasets/hype-de/hype/2019/'\n",
    "cols = hype_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/hype-de/hype/2019/hype.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process 24 hour data\n",
    "def hype24H_to_h5(dirpath):\n",
    "    hf = h5py.File(dirpath + 'hype24H.h5', 'w')\n",
    "    for s in os.listdir(dirpath):\n",
    "        if ('h5' not in s) & ('DS' not in s):\n",
    "            patient_base_path = dirpath + s + '/24 Hours'\n",
    "            sources = {}\n",
    "            if os.path.exists(patient_base_path+'/Empatica'):\n",
    "                sources['empatica'] = glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "            if os.path.exists(patient_base_path+'/SpaceLabs'):\n",
    "                sources['spacelabs'] = glob.glob(patient_base_path+r'/SpaceLabs*').pop() \n",
    "\n",
    "            if len(sources) > 0:\n",
    "                empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "                if os.path.exists(sources['spacelabs']):\n",
    "                    for file in os.listdir(sources['spacelabs']):\n",
    "                        if file.endswith(\".abp\"):\n",
    "                            spacelabsfile = os.path.join(sources['spacelabs'], file)\n",
    "                            break\n",
    "        #         print(spacelabsfile)\n",
    "                bp = devicely.SpacelabsReader(spacelabsfile)\n",
    "            #     bp.drop_EB()\n",
    "                bp.timeshift(pd.Timedelta(-2, unit='H'))\n",
    "\n",
    "                edata = empatica.data._get_numeric_data()\n",
    "                bpdata = bp.data._get_numeric_data()\n",
    "                subj_data = pd.DataFrame()\n",
    "                for i in range(len(bpdata.index)-1):\n",
    "                    e = edata[(edata.index >= bpdata.index[i]) & (edata.index < bpdata.index[i+1])]\n",
    "                    for col in bpdata.iloc[i].index:\n",
    "                        e[col] = bpdata.iloc[i][col]\n",
    "                        columns = e.columns\n",
    "                    subj_data = subj_data.append(e)\n",
    "                hf.create_dataset(s, data=subj_data.T)\n",
    "    hf.close()\n",
    "    return columns\n",
    "\n",
    "dirpath = '../../datasets/hype-de/hype/2019/'\n",
    "cols = hype24H_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/hype-de/hype/2019/hype24H.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1b915",
   "metadata": {},
   "source": [
    "# Non-invasive Blood Pressure Estimation ([dataset link](https://www.kaggle.com/datasets/mkachuee/noninvasivebp), [paper link](https://ieeexplore.ieee.org/document/8032000))\n",
    "\n",
    "Contains PCG, ECG, and PPG signals from 26 subjects. Additional information include age, weight, and height. Also contains signals from a force-sensing resistor (FSR) placed under the cuff BP device to distinguish exact moments of reference BP measurements.\n",
    "\n",
    "<a id=\"nibp\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mins(a, num_mins, window):\n",
    "    found_mins = []\n",
    "    amax = a.max()\n",
    "    hwindow = window // 2\n",
    "    a = np.array(a)\n",
    "    for i in range(num_mins):\n",
    "        found_min = np.argmin(a)\n",
    "        found_mins.append(found_min)\n",
    "        a[found_min-hwindow:found_min+hwindow] = amax\n",
    "    del a\n",
    "    return sorted(found_mins)\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def find_bp_measurement_points(data, plot):\n",
    "    data_FSR = -np.array(data['data_FSR'])\n",
    "    max_diff = 50\n",
    "    data_FSR_clear = np.array(data_FSR, dtype=np.float)\n",
    "    data_FSR_outliers = np.abs(data_FSR[1:] - data_FSR[:-1]) > max_diff\n",
    "    data_FSR_outliers = np.append(data_FSR_outliers, False)\n",
    "    data_FSR_clear[data_FSR_outliers] = np.nan\n",
    "    if plot == True:\n",
    "        plt.plot(1/np.array(data['data_FSR']))\n",
    "\n",
    "    mean_window = 10\n",
    "    data_FSR_roll_mean = np.nanmean(rolling_window(data_FSR_clear, mean_window), axis=-1)\n",
    "    data_FSR_clear[np.isnan(data_FSR_clear)] = \\\n",
    "        data_FSR_roll_mean[np.isnan(data_FSR_clear)[:1-mean_window]]\n",
    "    assert np.isnan(data_FSR_clear).sum() == 0\n",
    "    data_FSR_smooth = signal.savgol_filter(data_FSR_clear, 51, 0)\n",
    "\n",
    "    diff_n = 1000\n",
    "    roll_window = 21\n",
    "    data_FSR_diff = data_FSR_smooth[diff_n:] - data_FSR_smooth[:-diff_n]\n",
    "    data_FSR_diff_roll = rolling_window(data_FSR_diff, roll_window).mean(axis=-1)\n",
    "\n",
    "    num_mins = len(data['data_BP'])\n",
    "    min_window = 15000        \n",
    "    data_FSR_mins = find_mins(data_FSR_diff_roll, num_mins, min_window)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(data_FSR_smooth, label='Smoothed FSR')\n",
    "        data_FSR_max, data_FSR_min = data_FSR_smooth.max(), data_FSR_smooth.min()\n",
    "        for m in data_FSR_mins:\n",
    "            plt.vlines(m + diff_n/2, data_FSR_min, data_FSR_max, color='red')\n",
    "        plt.legend()\n",
    "        plt.title('BP measures points')\n",
    "    return data_FSR_mins\n",
    "\n",
    "def kachueeNIBPE_to_h5(dirpath):\n",
    "    data_keys = ['data_PPG', 'data_ECG', 'data_PCG', 'data_FSR']\n",
    "    hf = h5py.File(dirpath + 'kachueeNIBPE.h5', 'w')\n",
    "    for fname in os.listdir(dirpath):\n",
    "        if ('eval' not in fname) & ('h5' not in fname) & ('ipynb' not in fname):\n",
    "            with open(dirpath + fname, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            idxs = find_bp_measurement_points(data, plot=False)\n",
    "            data_keys = ['data_PPG', 'data_ECG', 'data_PCG', 'data_FSR']\n",
    "            data['data_BP'].append({'SBP': 0, 'DBP': 0})\n",
    "            d = pd.DataFrame()\n",
    "            for col in data_keys:\n",
    "                d[col] = data[col]\n",
    "            temp = np.append([0], idxs)\n",
    "            temp = np.append(temp, len(data['data_PPG']))\n",
    "            sbps = np.array([])\n",
    "            dbps = np.array([])\n",
    "            d['SBP'] = 0\n",
    "            d['DBP'] = 0\n",
    "            for i in range(len(temp)-1):\n",
    "                d.iloc[temp[i]:temp[i+1]]['SBP'] = data['data_BP'][i]['SBP']\n",
    "                d.iloc[temp[i]:temp[i+1]]['DBP'] = data['data_BP'][i]['DBP']\n",
    "            hf.create_dataset(fname.split('.')[0], data=d.to_numpy().T)\n",
    "    hf.close()\n",
    "    return d.columns\n",
    "\n",
    "dirpath = '../../datasets/kachueeNIBPE/'\n",
    "cols = kachueeNIBPE_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/kachueeNIBPE/kachueeNIBPE.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd887f6",
   "metadata": {},
   "source": [
    "# VitalDB ([dataset link](https://vitaldb.net/), [paper link](https://www.nature.com/articles/s41597-022-01411-5), [GitHub repo](https://github.com/vitaldb))\n",
    "\n",
    "Contains high-resolution multi-parameter data from 6388 surgical patients, including 486451 waveform and numeric data tracks of 196 intraoperative monitoring parameters, 73 perioperative clinical parameters, and 34 time-series laboratory result parameters.\n",
    "\n",
    "Not all data fields are simultaneously recorded. Therefore, in this example, we follow [Zhang et al., (2022)](https://iopscience.iop.org/article/10.1088/1361-6579/abf889/pdf) to extract continuous ECG and PPG data by keeping 5 minute windows with non-zero data and creating 8s segments. The data is downlaoded from the VitalDB database using the the VitalDB package. Note: here, not all data is downloaded - only the ones that contain ECG and PPG.\n",
    "\n",
    "<a id=\"vitaldb\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487798b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_nonzero_runs(a):\n",
    "    # Create an array that is 1 where a is nonzero, and pad each end with an extra 0.\n",
    "    isnonzero = np.concatenate(([0], (np.asarray(a) != 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(isnonzero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "def vitaldb_to_h5(dirpath, parameters, fsamp):\n",
    "    caseids = vitaldb.find_cases(parameters)\n",
    "    with h5py.File(dirpath + 'vitaldb_p1.h5', 'w') as hf:\n",
    "        for case in caseids:\n",
    "            print(case)\n",
    "            d = vitaldb.load_case(case, parameters, fsamp)\n",
    "\n",
    "            # filter data. keep 5 min windows with non-zero data\n",
    "            # find nonzero segments > 8s\n",
    "            d[np.isnan(d)] = 0\n",
    "            d[d[:, 2] <= 30] = 0\n",
    "            nonzero_segs = find_nonzero_runs(d[:, 0]*d[:, 1]*d[:, 2])\n",
    "            valid_segs = []\n",
    "            for seg in nonzero_segs:\n",
    "                # > 5mins then split into 8s segments\n",
    "                if seg[1]-seg[0] > 1/fsamp*60*5:\n",
    "                    idxs = np.arange(seg[0], seg[1], 1/fsamp*8)\n",
    "                    for i in range(len(idxs)-1):\n",
    "                        valid_segs.append([int(idxs[i]), int(idxs[i+1])])\n",
    "            subj_data = np.array([])\n",
    "            for seg in valid_segs:\n",
    "                if len(subj_data) == 0:\n",
    "                    subj_data = np.expand_dims(d[seg[0]:seg[1], :].T, axis=0)\n",
    "                else:\n",
    "                    subj_data = np.vstack((subj_data, np.expand_dims(d[seg[0]:seg[1], :].T, axis=0)))\n",
    "            hf.create_dataset(str(case), data=subj_data)\n",
    "    return caseids\n",
    "\n",
    "dirpath = '../../datasets/vitaldb/'\n",
    "parameters = parameters = ['SNUADC/PLETH', 'SNUADC/ECG_II', 'SNUADC/ART']\n",
    "fsamp = 1/125\n",
    "caseids = vitaldb_to_h5(dirpath, parameters, fsamp)\n",
    "dset = load_h5_dset('../../datasets/vitaldb/vitaldb_p1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f99579f",
   "metadata": {},
   "source": [
    "# Aurora-BP ([sample dataset link](https://github.com/microsoft/aurorabp-sample-data/main/sample), [full dataset request link](https://microsoft.na3.adobesign.com/public/esignWidget?wid=CBFCIBAA3AAABLblqZhD74UtFW8mtjvfuL24R-oLahbMHQd2OJTLURiy0cT8RXlTEFf3n5Y8OzpPdEPSiEvY*), [GitHub repo](https://github.com/microsoft/aurorabp-sample-data))\n",
    "\n",
    "Sample data contains [auscultatory](https://github.com/microsoft/aurorabp-sample-data/tree/main/sample/measurements_auscultatory) or [oscillometric](https://github.com/microsoft/aurorabp-sample-data/tree/main/sample/measurements_oscillometric) data of 5 subjects, including calibration data, exercise challenge, static challenge, static seated, and temporal challenge data. Also contains subject data information such as height, weight, and age. The full dataset also includes data from 1125 subjects and 24-hour BP monitoring data.\n",
    "\n",
    "The data can be processed in many different ways. Here, we place subject data from each stage (i.e. calibration, exercise challenge) into separate subdicts.\n",
    "\n",
    "<a id=\"aurora\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aurorabp_to_h5(dirpath):\n",
    "    hf = h5py.File(dirpath + 'aurorabp.h5', 'w')\n",
    "    for subj in os.listdir(dirpath):\n",
    "        for fname in os.listdir(dirpath + subj):\n",
    "            d = pd.read_csv(dirpath + subj + '/' + fname, delimited='\\t')\n",
    "            hf.create_dataset(fname.split('.tsv')[0], data=d.T)\n",
    "            cols = d.columns\n",
    "    hf.close()\n",
    "    return cols\n",
    "\n",
    "dirpath = '~/datasets/aurorabp-sample-data/sample/measurements_auscultatory/'\n",
    "cols = aurorabp_to_h5(dirpath)\n",
    "dset = load_h5_dset('../../datasets/aurorabp/aurorabp.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
