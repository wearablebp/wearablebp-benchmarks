{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8d6dc4",
   "metadata": {},
   "source": [
    "This notebook contains examples of how to convert different datasets into .h5 format. We choose to use the .h5 format because data can be accessed far quicker than other formats that require loading the whole dataset into memory. For more information about available datasets, see our [website page](https://wearablebp.github.io/datasets) on publicly available datasets.\n",
    "\n",
    "The examples below creates segments of arbitrary size. Each reshaped record is stacked together in the form [segment number, sensor data number, segment length]. For example, in the MIMIC-II dataset from UCI Repository, record 0 will be saved in the same [21, 3, 128] where 21 is the number of segments, 3 is the number of signals (ECG, PPG, ABP), and 128 is the segment length. Data can be retrieved by specifying '<record number>' and indexing (example: `hf['0'][0, :, :]`). The record numbers can be viewed using the command `hf.keys()`.   \n",
    "   \n",
    "1. [MIMIC-II from UCI Repository](#mimic)\n",
    "2. [PPG-BP](#ppgbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e6dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special packages used for particular datasets\n",
    "!pip install openpyxl\n",
    "!pip install devicely\n",
    "!pip install glob2\n",
    "!pip install wfdb\n",
    "!pip install vitaldb\n",
    "\n",
    "import glob\n",
    "import devicely\n",
    "import vitaldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d9de061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_dset(datapath):\n",
    "    dset = h5py.File(datapath, 'r')\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93aeac",
   "metadata": {},
   "source": [
    "# MIMIC-II from UCI Repository ([dataset link](https://archive.ics.uci.edu/ml/datasets/Cuff-Less+Blood+Pressure+Estimation), [paper link](https://ieeexplore.ieee.org/document/7491263))\n",
    "\n",
    "\n",
    "\n",
    "Contains raw ECG, PPG, and ABP signals from the physionet [MIMIC-II Waveform Database](https://archive.physionet.org/physiobank/database/mimic2wdb/) processed by [Kachuee et al., (2015)](https://ieeexplore.ieee.org/document/7491263). Which signals contain signals from the same patients is unclear. In the paper, approximately 1 in 12 signals come from the same person. A [follow up work](https://ieeexplore.ieee.org/document/8938751) with a co-author also used this dataset and reduces the data leakage by shuffling and random sampling.\n",
    "\n",
    "<a id='mimic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd71adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../datasets/MIMIC-II/Part1234.mat'\n",
    "dset = load_h5_dset(datapath)\n",
    "segment_length = 128\n",
    "\n",
    "with h5py.File('../datasets/MIMIC-II/kachuee17_' + str(int(segment_length)) + '.h5', 'w') as hf:\n",
    "    keys = list(dset.keys())\n",
    "    for k in range(len(keys)):\n",
    "        subj_data = []\n",
    "        if dset[keys[k]][:].shape[1] >= segment_length:\n",
    "            for i in range(0, segment_length*(dset[keys[k]][:].shape[1]//segment_length), segment_length):\n",
    "                subj_data.append(dset[keys[k]][:, i:i+segment_length])\n",
    "        if len(subj_data) > 0:\n",
    "            hf.create_dataset(keys[k], data=np.array(subj_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e14d423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"0\": shape (21, 3, 1000), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('../datasets/MIMIC-II/kachuee17_1000.h5', 'r') as hf:\n",
    "    print(hf['0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5837c",
   "metadata": {},
   "source": [
    "# PPG-BP Dataset ([dataset link](https://figshare.com/articles/dataset/PPG-BP_Database_zip/5459299), [paper link](https://www.nature.com/articles/sdata201820))\n",
    "\n",
    "Contains ~2.1s PPG data and cuff BP measurements and demographic information. Cuff BP measurements are contained in 'Table 1.xlsx'. The PPG data is filtered using SQI metrics from [Liang et al., (2018)](https://www.nature.com/articles/sdata201876).\n",
    "\n",
    "Because .h5 files store data in arrays, the data need to be matched in all dimensions. Since cuff BP measurements are only reported once for each subject, an array of size <segment length> is created with the first half of values as SBP and second half of values as DBP.\n",
    "\n",
    "<a id='ppgbp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppgbp_to_h5(dirpath):\n",
    "\n",
    "    fnames = os.listdir(dirpath)\n",
    "    subjs = []\n",
    "    for fname in fnames:\n",
    "        subjs.append(fname.split('_')[0])\n",
    "    unique_subjs = np.unique(subjs)\n",
    "\n",
    "    gt = pd.read_excel(dirpath + '../' + 'PPG-BP dataset.xlsx', header=1)\n",
    "    subj_ids = gt['subject_ID'].astype(str).values\n",
    "    sbp_gt = gt['Systolic Blood Pressure(mmHg)'].values\n",
    "    dbp_gt = gt['Diastolic Blood Pressure(mmHg)'].values\n",
    "\n",
    "    with h5py.File('../datasets/PPG-BP/liang18.h5', 'w') as hf:\n",
    "        for subj in unique_subjs:\n",
    "            subj_data = np.array([])\n",
    "            for filename in os.listdir(dirpath):\n",
    "                s = filename.split('_')\n",
    "                if s[0] == subj:\n",
    "                    record = s[1].split('.')[0]\n",
    "                    d = np.loadtxt(dirpath + filename)\n",
    "                    d = np.expand_dims(d, axis=0)\n",
    "                    s = d.shape\n",
    "                    d = np.stack((d, np.hstack((np.ones((1, s[1]//2))*sbp_gt[np.where(subj_ids == subj)], np.ones((1, s[1]//2))*dbp_gt[np.where(subj_ids == subj)]))), axis=1)\n",
    "                    if ~subj_data.any():\n",
    "                        subj_data = d\n",
    "                    else:\n",
    "                        if subj_data.shape[1] == len(d):\n",
    "                            subj_data = np.vstack((subj_data, d))\n",
    "                        else:\n",
    "                            for i in range(0, len(d), 2100):\n",
    "                                subj_data = np.vstack((subj_data, d[:, :, i:i+2100]))\n",
    "            hf.create_dataset(subj, data=subj_data)\n",
    "    \n",
    "dirpath = '../datasets/PPG-BP/0_subject/'\n",
    "ppgbp_to_h5(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "271dc93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"10\": shape (3, 2, 2100), type \"<f8\">"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../datasets/PPG-BP/liang18.h5'\n",
    "dset = load_h5_dset(datapath)\n",
    "dset['10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252de32c",
   "metadata": {},
   "source": [
    "# University of Queensland Vital Signs Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce257519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uoq_to_h5(dirpath):\n",
    "    \n",
    "    numeric_cols = np.array([])\n",
    "    for case in os.listdir(dirpath):\n",
    "        if 'case' in case:\n",
    "            fulldata_dir = dirpath + case + '/fulldata/'\n",
    "            subj_data = np.array([])\n",
    "            for f in os.listdir(fulldata_dir):\n",
    "                d = pd.read_csv(fulldata_dir + f, error_bad_lines=False)\n",
    "                columns = d.columns\n",
    "                d = d._get_numeric_data()\n",
    "                numeric_cols = np.append(numeric_cols, d.columns)\n",
    "    numeric_cols = np.unique(numeric_cols)\n",
    "    \n",
    "    hf = h5py.File(dirpath + 'uoq_dset.h5', 'w')\n",
    "    for case in os.listdir(dirpath):\n",
    "        if 'case' in case:\n",
    "            fulldata_dir = dirpath + case + '/fulldata/'\n",
    "            for f in os.listdir(fulldata_dir):\n",
    "                d = pd.read_csv(fulldata_dir + f, error_bad_lines=False)\n",
    "                columns = d.columns\n",
    "                d = d[numeric_cols]\n",
    "                for col in d.columns:\n",
    "                    d[col] = pd.to_numeric(d[col], errors='coerce')\n",
    "            hf.create_dataset(case + '_' + f.split('_')[-1].split('.')[0], data=d.T)\n",
    "    hf.close()\n",
    "\n",
    "dirpath = '../datasets/uqvitalsignsdata/'\n",
    "uoq_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/uqvitalsignsdata/uoq_dset.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pttppg_to_h5(dirpath)\n",
    "    numeric_cols = np.array([])\n",
    "    for s in unique_subjs:\n",
    "        for f in unique_fnames:\n",
    "            if 's' + s + '_' in f:\n",
    "                d = pd.read_csv(dirpath + f)\n",
    "                d = d._get_numeric_data()\n",
    "                columns = d.columns\n",
    "            numeric_cols = np.append(numeric_cols, d.columns)\n",
    "    numeric_cols = np.unique(numeric_cols)\n",
    "\n",
    "    hf = h5py.File(dirpath + 'pttppg.h5', 'w')\n",
    "    for s in unique_subjs:\n",
    "        for f in unique_fnames:\n",
    "            if 's' + s + '_' in f:\n",
    "                d = pd.read_csv(dirpath + f)\n",
    "                d = d[numeric_cols]\n",
    "                for col in d.columns:\n",
    "                    d[col] = pd.to_numeric(d[col], errors='coerce')\n",
    "                hf.create_dataset(f.split('.')[0], data=d.T)\n",
    "    hf.close()  \n",
    "    return numeric_cols\n",
    "\n",
    "dirpath = '../datasets/pulse-transit-time-ppg/1.1.0/csv/'\n",
    "cols = pttppg_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/pulse-transit-time-ppg/1.1.0/csv/pttppg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd95b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charis_to_h5(dirpath):\n",
    "    fnames = np.array([])\n",
    "    for f in os.listdir(dirpath):\n",
    "        if ('charis' in f) & ('h5' not in f):\n",
    "            fnames = np.append(fnames, f.split('.')[0])\n",
    "    unique_fnames = np.unique(fnames)\n",
    "\n",
    "    hf = h5py.File(dirpath + 'charis.h5', 'w')\n",
    "    for f in unique_fnames:\n",
    "        if ('charis' in f) & ('h5' not in f):\n",
    "            signals, fields = wfdb.rdsamp(dirpath + f.split('.')[0])\n",
    "            hf.create_dataset(f.split('.')[0].split('charis')[1], data=signals.T)\n",
    "    hf.close()\n",
    "    \n",
    "dirpath = '../datasets/charisdb/1.0.0/'\n",
    "charis_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/charisdb/1.0.0/charis.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16027905",
   "metadata": {},
   "source": [
    "# HYPE\n",
    "\n",
    "Some code from: https://github.com/arianesasso/aime-2020/blob/master/notebooks/processing_hype_for_3_3/json_ppg_bp_window_hype.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hype_to_h5(dirpath):\n",
    "    hf = h5py.File(dirpath + 'hype.h5', 'w')\n",
    "    for s in os.listdir(dirpath):\n",
    "        if ('h5' not in s) & ('DS' not in s):\n",
    "            patient_base_path = dirpath + s\n",
    "            sources = {}\n",
    "            if os.path.exists(patient_base_path+'/Tag'):\n",
    "                sources['tag'] = glob.glob(patient_base_path+r'/Tag*').pop()\n",
    "            if os.path.exists(patient_base_path+'/Empatica'):\n",
    "                sources['empatica'] = glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "            if os.path.exists(patient_base_path+'/SpaceLabs'):\n",
    "                sources['spacelabs'] = glob.glob(patient_base_path+r'/SpaceLabs*').pop() \n",
    "            \n",
    "            if len(sources) > 0:\n",
    "                empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "                if os.path.exists(sources['spacelabs']):\n",
    "                    for file in os.listdir(sources['spacelabs']):\n",
    "                        if file.endswith(\".abp\"):\n",
    "                            spacelabsfile = os.path.join(sources['spacelabs'], file)\n",
    "                            break\n",
    "        #         print(spacelabsfile)\n",
    "                bp = devicely.SpacelabsReader(spacelabsfile)\n",
    "            #     bp.drop_EB()\n",
    "                bp.timeshift(pd.Timedelta(-2, unit='H'))\n",
    "\n",
    "                edata = empatica.data._get_numeric_data()\n",
    "                bpdata = bp.data._get_numeric_data()\n",
    "                subj_data = pd.DataFrame()\n",
    "                for i in range(len(bpdata.index)-1):\n",
    "                    e = edata[(edata.index >= bpdata.index[i]) & (edata.index < bpdata.index[i+1])]\n",
    "                    for col in bpdata.iloc[i].index:\n",
    "                        e[col] = bpdata.iloc[i][col]\n",
    "                        columns = e.columns\n",
    "                    subj_data = subj_data.append(e)\n",
    "                hf.create_dataset(s, data=subj_data.T)\n",
    "    return columns\n",
    "\n",
    "dirpath = '../datasets/hype-de/hype/2019/'\n",
    "cols = hype_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/hype-de/hype/2019/hype.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hype24H_to_h5(dirpath):\n",
    "    hf = h5py.File(dirpath + 'hype24H.h5', 'w')\n",
    "    for s in os.listdir(dirpath):\n",
    "        if ('h5' not in s) & ('DS' not in s):\n",
    "            patient_base_path = dirpath + s + '/24 Hours'\n",
    "            sources = {}\n",
    "            if os.path.exists(patient_base_path+'/Empatica'):\n",
    "                sources['empatica'] = glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "            if os.path.exists(patient_base_path+'/SpaceLabs'):\n",
    "                sources['spacelabs'] = glob.glob(patient_base_path+r'/SpaceLabs*').pop() \n",
    "\n",
    "            if len(sources) > 0:\n",
    "                empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "                if os.path.exists(sources['spacelabs']):\n",
    "                    for file in os.listdir(sources['spacelabs']):\n",
    "                        if file.endswith(\".abp\"):\n",
    "                            spacelabsfile = os.path.join(sources['spacelabs'], file)\n",
    "                            break\n",
    "        #         print(spacelabsfile)\n",
    "                bp = devicely.SpacelabsReader(spacelabsfile)\n",
    "            #     bp.drop_EB()\n",
    "                bp.timeshift(pd.Timedelta(-2, unit='H'))\n",
    "\n",
    "                edata = empatica.data._get_numeric_data()\n",
    "                bpdata = bp.data._get_numeric_data()\n",
    "                subj_data = pd.DataFrame()\n",
    "                for i in range(len(bpdata.index)-1):\n",
    "                    e = edata[(edata.index >= bpdata.index[i]) & (edata.index < bpdata.index[i+1])]\n",
    "                    for col in bpdata.iloc[i].index:\n",
    "                        e[col] = bpdata.iloc[i][col]\n",
    "                        columns = e.columns\n",
    "                    subj_data = subj_data.append(e)\n",
    "                hf.create_dataset(s, data=subj_data.T)\n",
    "    hf.close()\n",
    "    return columns\n",
    "\n",
    "dirpath = '../datasets/hype-de/hype/2019/'\n",
    "cols = hype24H_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/hype-de/hype/2019/hype24H.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1b915",
   "metadata": {},
   "source": [
    "# Non-invasive Blood Pressure Estimation\n",
    "https://www.kaggle.com/datasets/mkachuee/noninvasivebp\n",
    "\n",
    "BP measured at time stamps\n",
    "\n",
    "BP measurement is assigned as signals(before time stamp) = BP(time stamp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mins(a, num_mins, window):\n",
    "    found_mins = []\n",
    "    amax = a.max()\n",
    "    hwindow = window // 2\n",
    "    a = np.array(a)\n",
    "    for i in range(num_mins):\n",
    "        found_min = np.argmin(a)\n",
    "        found_mins.append(found_min)\n",
    "        a[found_min-hwindow:found_min+hwindow] = amax\n",
    "    del a\n",
    "    return sorted(found_mins)\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def find_bp_measurement_points(data, plot):\n",
    "    data_FSR = -np.array(data['data_FSR'])\n",
    "    max_diff = 50\n",
    "    data_FSR_clear = np.array(data_FSR, dtype=np.float)\n",
    "    data_FSR_outliers = np.abs(data_FSR[1:] - data_FSR[:-1]) > max_diff\n",
    "    data_FSR_outliers = np.append(data_FSR_outliers, False)\n",
    "    data_FSR_clear[data_FSR_outliers] = np.nan\n",
    "    if plot == True:\n",
    "        plt.plot(1/np.array(data['data_FSR']))\n",
    "\n",
    "    mean_window = 10\n",
    "    data_FSR_roll_mean = np.nanmean(rolling_window(data_FSR_clear, mean_window), axis=-1)\n",
    "    data_FSR_clear[np.isnan(data_FSR_clear)] = \\\n",
    "        data_FSR_roll_mean[np.isnan(data_FSR_clear)[:1-mean_window]]\n",
    "    assert np.isnan(data_FSR_clear).sum() == 0\n",
    "    data_FSR_smooth = signal.savgol_filter(data_FSR_clear, 51, 0)\n",
    "\n",
    "    diff_n = 1000\n",
    "    roll_window = 21\n",
    "    data_FSR_diff = data_FSR_smooth[diff_n:] - data_FSR_smooth[:-diff_n]\n",
    "    data_FSR_diff_roll = rolling_window(data_FSR_diff, roll_window).mean(axis=-1)\n",
    "\n",
    "    num_mins = len(data['data_BP'])\n",
    "    min_window = 15000        \n",
    "    data_FSR_mins = find_mins(data_FSR_diff_roll, num_mins, min_window)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(data_FSR_smooth, label='Smoothed FSR')\n",
    "        data_FSR_max, data_FSR_min = data_FSR_smooth.max(), data_FSR_smooth.min()\n",
    "        for m in data_FSR_mins:\n",
    "            plt.vlines(m + diff_n/2, data_FSR_min, data_FSR_max, color='red')\n",
    "        plt.legend()\n",
    "        plt.title('BP measures points')\n",
    "    return data_FSR_mins\n",
    "\n",
    "def kachueeNIBPE_to_h5(dirpath):\n",
    "    data_keys = ['data_PPG', 'data_ECG', 'data_PCG', 'data_FSR']\n",
    "    hf = h5py.File(dirpath + 'kachueeNIBPE.h5', 'w')\n",
    "    for fname in os.listdir(dirpath):\n",
    "        if ('eval' not in fname) & ('h5' not in fname) & ('ipynb' not in fname):\n",
    "            with open(dirpath + fname, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            idxs = find_bp_measurement_points(data, plot=False)\n",
    "            data_keys = ['data_PPG', 'data_ECG', 'data_PCG', 'data_FSR']\n",
    "            data['data_BP'].append({'SBP': 0, 'DBP': 0})\n",
    "            d = pd.DataFrame()\n",
    "            for col in data_keys:\n",
    "                d[col] = data[col]\n",
    "            temp = np.append([0], idxs)\n",
    "            temp = np.append(temp, len(data['data_PPG']))\n",
    "            sbps = np.array([])\n",
    "            dbps = np.array([])\n",
    "            d['SBP'] = 0\n",
    "            d['DBP'] = 0\n",
    "            for i in range(len(temp)-1):\n",
    "                d.iloc[temp[i]:temp[i+1]]['SBP'] = data['data_BP'][i]['SBP']\n",
    "                d.iloc[temp[i]:temp[i+1]]['DBP'] = data['data_BP'][i]['DBP']\n",
    "            hf.create_dataset(fname.split('.')[0], data=d.to_numpy().T)\n",
    "    hf.close()\n",
    "    return d.columns\n",
    "\n",
    "dirpath = '../datasets/kachueeNIBPE/'\n",
    "cols = kachueeNIBPE_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/kachueeNIBPE/kachueeNIBPE.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd887f6",
   "metadata": {},
   "source": [
    "# VitalDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487798b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n",
      "7\n",
      "10\n",
      "13\n",
      "14\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "22\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "31\n",
      "32\n",
      "34\n",
      "38\n",
      "43\n",
      "44\n",
      "46\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "74\n",
      "75\n",
      "77\n",
      "79\n",
      "83\n",
      "84\n",
      "87\n",
      "89\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "96\n",
      "97\n",
      "98\n",
      "101\n",
      "103\n",
      "104\n",
      "105\n",
      "108\n",
      "110\n",
      "111\n",
      "112\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "124\n",
      "125\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "142\n",
      "143\n",
      "145\n",
      "148\n",
      "149\n",
      "150\n",
      "152\n",
      "156\n",
      "157\n",
      "160\n",
      "161\n",
      "163\n",
      "164\n",
      "166\n",
      "167\n",
      "168\n",
      "172\n",
      "175\n",
      "177\n",
      "178\n",
      "180\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "189\n",
      "190\n",
      "191\n",
      "195\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "202\n",
      "203\n",
      "206\n",
      "207\n",
      "208\n",
      "210\n",
      "217\n",
      "218\n",
      "221\n",
      "222\n",
      "229\n",
      "230\n",
      "232\n",
      "233\n",
      "234\n",
      "236\n",
      "237\n",
      "239\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "246\n",
      "247\n",
      "250\n",
      "252\n",
      "256\n",
      "258\n",
      "261\n",
      "263\n",
      "266\n",
      "268\n",
      "269\n",
      "270\n",
      "272\n",
      "279\n",
      "281\n",
      "282\n",
      "283\n",
      "286\n",
      "287\n",
      "293\n",
      "295\n",
      "296\n",
      "297\n",
      "300\n",
      "302\n",
      "303\n",
      "304\n",
      "306\n",
      "308\n",
      "309\n",
      "312\n",
      "316\n",
      "318\n",
      "319\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "330\n",
      "332\n",
      "337\n",
      "338\n",
      "343\n",
      "345\n",
      "347\n",
      "348\n",
      "351\n",
      "353\n",
      "354\n",
      "355\n",
      "357\n",
      "358\n",
      "359\n",
      "362\n",
      "363\n",
      "364\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "375\n",
      "377\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "390\n",
      "402\n",
      "404\n",
      "405\n",
      "406\n",
      "408\n",
      "409\n",
      "413\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "424\n",
      "425\n",
      "427\n",
      "435\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "445\n",
      "447\n",
      "448\n",
      "449\n",
      "451\n",
      "452\n",
      "455\n",
      "458\n",
      "460\n",
      "462\n",
      "463\n",
      "466\n",
      "468\n",
      "469\n",
      "472\n",
      "476\n",
      "477\n",
      "478\n",
      "481\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "488\n",
      "489\n",
      "490\n",
      "492\n",
      "495\n",
      "497\n",
      "499\n",
      "501\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "512\n",
      "513\n",
      "514\n",
      "516\n",
      "520\n",
      "521\n",
      "524\n",
      "526\n",
      "527\n",
      "530\n",
      "531\n",
      "533\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "541\n",
      "543\n",
      "544\n",
      "545\n",
      "547\n",
      "550\n",
      "551\n",
      "557\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "570\n",
      "573\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "587\n",
      "589\n",
      "590\n",
      "593\n",
      "594\n",
      "596\n",
      "599\n",
      "611\n",
      "612\n",
      "616\n",
      "617\n",
      "620\n",
      "621\n",
      "622\n",
      "624\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "631\n",
      "634\n",
      "636\n",
      "637\n",
      "638\n",
      "641\n",
      "644\n",
      "645\n",
      "648\n",
      "649\n",
      "650\n",
      "652\n",
      "654\n",
      "655\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "663\n",
      "665\n",
      "666\n",
      "667\n",
      "671\n",
      "673\n",
      "676\n",
      "678\n",
      "679\n",
      "680\n",
      "683\n",
      "684\n",
      "685\n",
      "687\n",
      "688\n",
      "689\n",
      "691\n",
      "693\n",
      "697\n",
      "698\n",
      "699\n",
      "702\n",
      "703\n",
      "707\n",
      "711\n",
      "716\n",
      "719\n",
      "721\n",
      "722\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "733\n",
      "734\n",
      "735\n",
      "737\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "772\n",
      "773\n",
      "774\n",
      "776\n",
      "777\n",
      "779\n",
      "781\n",
      "788\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "800\n",
      "802\n",
      "807\n",
      "808\n",
      "810\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "819\n",
      "822\n",
      "825\n",
      "827\n",
      "830\n",
      "831\n",
      "833\n",
      "835\n",
      "841\n",
      "843\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "859\n",
      "860\n",
      "864\n",
      "865\n",
      "866\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "876\n",
      "879\n",
      "880\n",
      "881\n",
      "883\n",
      "885\n",
      "886\n",
      "887\n",
      "890\n",
      "892\n",
      "894\n",
      "897\n",
      "898\n",
      "907\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "916\n",
      "917\n",
      "919\n",
      "922\n",
      "925\n",
      "926\n",
      "931\n",
      "932\n",
      "933\n"
     ]
    }
   ],
   "source": [
    "def find_nonzero_runs(a):\n",
    "    # Create an array that is 1 where a is nonzero, and pad each end with an extra 0.\n",
    "    isnonzero = np.concatenate(([0], (np.asarray(a) != 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(isnonzero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "def vitaldb_to_h5(dirpath, parameters, fsamp):\n",
    "    caseids = vitaldb.find_cases(parameters)\n",
    "    with h5py.File(dirpath + 'vitaldb_p1.h5', 'w') as hf:\n",
    "        for case in caseids[:500]:\n",
    "            print(case)\n",
    "            d = vitaldb.load_case(case, parameters, fsamp)\n",
    "\n",
    "            # filter data. keep 5 min windows with non-zero data\n",
    "            # from https://iopscience.iop.org/article/10.1088/1361-6579/abf889/pdf\n",
    "            # find nonzero segments > 8s\n",
    "            d[np.isnan(d)] = 0\n",
    "            d[d[:, 2] <= 30] = 0\n",
    "            nonzero_segs = find_nonzero_runs(d[:, 0]*d[:, 1]*d[:, 2])\n",
    "            valid_segs = []\n",
    "            for seg in nonzero_segs:\n",
    "                # > 5mins then split into 8s segments\n",
    "                if seg[1]-seg[0] > 1/fsamp*60*5:\n",
    "                    idxs = np.arange(seg[0], seg[1], 1/fsamp*8)\n",
    "                    for i in range(len(idxs)-1):\n",
    "                        valid_segs.append([int(idxs[i]), int(idxs[i+1])])\n",
    "            subj_data = np.array([])\n",
    "            for seg in valid_segs:\n",
    "                if len(subj_data) == 0:\n",
    "                    subj_data = np.expand_dims(d[seg[0]:seg[1], :].T, axis=0)\n",
    "                else:\n",
    "                    subj_data = np.vstack((subj_data, np.expand_dims(d[seg[0]:seg[1], :].T, axis=0)))\n",
    "            hf.create_dataset(str(case), data=subj_data)\n",
    "    return caseids\n",
    "\n",
    "dirpath = '../datasets/vitaldb/'\n",
    "parameters = parameters = ['SNUADC/PLETH', 'SNUADC/ECG_II', 'SNUADC/ART']\n",
    "fsamp = 1/125\n",
    "caseids = vitaldb_to_h5(dirpath, parameters, fsamp)\n",
    "dset = load_h5_dset('../datasets/vitaldb/vitaldb_p1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f99579f",
   "metadata": {},
   "source": [
    "# Aurora-BP\n",
    "sample data from https://github.com/microsoft/aurorabp-sample-data/tree/main/sample/measurements_auscultatory or oscillometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aurorabp_to_h5(dirpath):\n",
    "    hf = h5py.File(dirpath + 'aurorabp.h5', 'w')\n",
    "    for subj in os.listdir(dirpath):\n",
    "        for fname in os.listdir(dirpath + subj):\n",
    "            d = pd.read_csv(dirpath + subj + '/' + fname, delimited='\\t')\n",
    "            hf.create_dataset(fname.split('.tsv')[0], data=d.T)\n",
    "            cols = d.columns\n",
    "    hf.close()\n",
    "    return cols\n",
    "\n",
    "dirpath = '~/datasets/aurorabp-sample-data/sample/measurements_auscultatory/'\n",
    "cols = aurorabp_to_h5(dirpath)\n",
    "dset = load_h5_dset('../datasets/aurorabp/aurorabp.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
